{"node_id": "757c2bf4", "title": "6. Build and compile the agent", "path": "quickstart > Quickstart > 6. Build and compile the agent", "content": "The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.compile) method.\n\n    ```python  theme={null}\n    # Build workflow\n    agent_builder = StateGraph(MessagesState)\n\n    # Add nodes\n    agent_builder.add_node(\"llm_call\", llm_call)\n    agent_builder.add_node(\"tool_node\", tool_node)\n\n    # Add edges to connect nodes\n    agent_builder.add_edge(START, \"llm_call\")\n    agent_builder.add_conditional_edges(\n        \"llm_call\",\n        should_continue,\n        [\"tool_node\", END]\n    )\n    agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n    # Compile the agent\n    agent = agent_builder.compile()\n\n    # Show the agent\n    from IPython.display import Image, display\n    display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n    # Invoke\n    from langchain.messages import HumanMessage\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    messages = agent.invoke({\"messages\": messages})\n    for m in messages[\"messages\"]:\n        m.pretty_print()\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Graph API.\n\n    <Accordion title=\"Full code example\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      # Step 2: Define state\n\n      from langchain.messages import AnyMessage\n      from typing_extensions import TypedDict, Annotated\n      import operator\n\n\n      class MessagesState(TypedDict):\n          messages: Annotated[list[AnyMessage], operator.add]\n          llm_calls: int\n\n      # Step 3: Define model node\n      from langchain.messages import SystemMessage\n\n\n      def llm_call(state: dict):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n          return {\n              \"messages\": [\n                  model_with_tools.invoke(\n                      [\n                          SystemMessage(\n                              content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                          )\n                      ]\n                      + state[\"messages\"]\n                  )\n              ],\n              \"llm_calls\": state.get('llm_calls', 0) + 1\n          }\n\n\n      # Step 4: Define tool node\n\n      from langchain.messages import ToolMessage\n\n\n      def tool_node(state: dict):\n          \"\"\"Performs the tool call\"\"\"\n\n          result = []\n          for tool_call in state[\"messages\"][-1].tool_calls:\n              tool = tools_by_name[tool_call[\"name\"]]\n              observation = tool.invoke(tool_call[\"args\"])\n              result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n          return {\"messages\": result}\n\n      # Step 5: Define logic to determine whether to end\n\n      from typing import Literal\n      from langgraph.graph import StateGraph, START, END\n\n\n      # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n      def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n          \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n          messages = state[\"messages\"]\n          last_message = messages[-1]\n\n          # If the LLM makes a tool call, then perform an action\n          if last_message.tool_calls:\n              return \"tool_node\"\n\n          # Otherwise, we stop (reply to the user)\n          return END\n\n      # Step 6: Build agent\n\n      # Build workflow\n      agent_builder = StateGraph(MessagesState)\n\n      # Add nodes\n      agent_builder.add_node(\"llm_call\", llm_call)\n      agent_builder.add_node(\"tool_node\", tool_node)\n\n      # Add edges to connect nodes\n      agent_builder.add_edge(START, \"llm_call\")\n      agent_builder.add_conditional_edges(\n          \"llm_call\",\n          should_continue,\n          [\"tool_node\", END]\n      )\n      agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n      # Compile the agent\n      agent = agent_builder.compile()\n\n\n      from IPython.display import Image, display\n      # Show the agent\n      display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n      # Invoke\n      from langchain.messages import HumanMessage\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      messages = agent.invoke({\"messages\": messages})\n      for m in messages[\"messages\"]:\n          m.pretty_print()\n\n      ```\n    </Accordion>\n  </Tab>\n\n  <Tab title=\"Use the Functional API\">", "summary": "Demonstrates building and compiling a LangGraph agent using StateGraph, adding nodes and edges, and invoking it with a sample query.", "keywords": ["StateGraph", "compile", "add_node", "add_edge", "add_conditional_edges", "MessagesState", "llm_call", "tool_node", "should_continue", "invoke"]}