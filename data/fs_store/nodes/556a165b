{"node_id": "556a165b", "title": "Agents", "path": "workflows-agents > Workflows and agents > Agents", "content": "Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd8da41dbf8b5e6fc9ea6bb10cb63e38\" alt=\"agent.png\" data-og-width=\"1732\" width=\"1732\" data-og-height=\"712\" height=\"712\" data-path=\"oss/images/agent.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=280&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=f7a590604edc49cfa273b5856f3a3ee3 280w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=560&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=dff9b17d345fe0fea25616b3b0dc6ebf 560w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=840&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=bd932835b919f5e58be77221b6d0f194 840w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1100&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=d53318b0c9c898a6146991691cbac058 1100w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=1650&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=ea66fb96bc07c595d321b8b71e651ddb 1650w, https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/agent.png?w=2500&fit=max&auto=format&n=-_xGPoyjhyiDWTPJ&q=85&s=b02599a3c9ba2a5c830b9a346f9d26c9 2500w\" />\n\n<Note>\n  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.\n</Note>\n\n```python Using tools theme={null}\nfrom langchain.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -> int:\n    \"\"\"Multiply `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -> int:\n    \"\"\"Adds `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -> float:\n    \"\"\"Divide `a` and `b`.\n\n    Args:\n        a: First int\n        b: Second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n```\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from langgraph.graph import MessagesState\n  from langchain.messages import SystemMessage, HumanMessage, ToolMessage\n\n\n  # Nodes\n  def llm_call(state: MessagesState):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n      return {\n          \"messages\": [\n              llm_with_tools.invoke(\n                  [\n                      SystemMessage(\n                          content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                      )\n                  ]\n                  + state[\"messages\"]\n              )\n          ]\n      }\n\n\n  def tool_node(state: dict):\n      \"\"\"Performs the tool call\"\"\"\n\n      result = []\n      for tool_call in state[\"messages\"][-1].tool_calls:\n          tool = tools_by_name[tool_call[\"name\"]]\n          observation = tool.invoke(tool_call[\"args\"])\n          result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n      return {\"messages\": result}\n\n\n  # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n  def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n      \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n      messages = state[\"messages\"]\n      last_message = messages[-1]\n\n      # If the LLM makes a tool call, then perform an action\n      if last_message.tool_calls:\n          return \"tool_node\"\n\n      # Otherwise, we stop (reply to the user)\n      return END\n\n\n  # Build workflow\n  agent_builder = StateGraph(MessagesState)\n\n  # Add nodes\n  agent_builder.add_node(\"llm_call\", llm_call)\n  agent_builder.add_node(\"tool_node\", tool_node)\n\n  # Add edges to connect nodes\n  agent_builder.add_edge(START, \"llm_call\")\n  agent_builder.add_conditional_edges(\n      \"llm_call\",\n      should_continue,\n      [\"tool_node\", END]\n  )\n  agent_builder.add_edge(\"tool_node\", \"llm_call\")\n\n  # Compile the agent\n  agent = agent_builder.compile()\n\n  # Show the agent\n  display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n  # Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  messages = agent.invoke({\"messages\": messages})\n  for m in messages[\"messages\"]:\n      m.pretty_print()\n  ```\n\n  ```python Functional API theme={null}\n  from langgraph.graph import add_messages\n  from langchain.messages import (\n      SystemMessage,\n      HumanMessage,\n      ToolCall,\n  )\n  from langchain_core.messages import BaseMessage\n\n\n  @task\n  def call_llm(messages: list[BaseMessage]):\n      \"\"\"LLM decides whether to call a tool or not\"\"\"\n      return llm_with_tools.invoke(\n          [\n              SystemMessage(\n                  content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n              )\n          ]\n          + messages\n      )\n\n\n  @task\n  def call_tool(tool_call: ToolCall):\n      \"\"\"Performs the tool call\"\"\"\n      tool = tools_by_name[tool_call[\"name\"]]\n      return tool.invoke(tool_call)\n\n\n  @entrypoint()\n  def agent(messages: list[BaseMessage]):\n      llm_response = call_llm(messages).result()\n\n      while True:\n          if not llm_response.tool_calls:\n              break\n\n          # Execute tools\n          tool_result_futures = [\n              call_tool(tool_call) for tool_call in llm_response.tool_calls\n          ]\n          tool_results = [fut.result() for fut in tool_result_futures]\n          messages = add_messages(messages, [llm_response, *tool_results])\n          llm_response = call_llm(messages).result()\n\n      messages = add_messages(messages, llm_response)\n      return messages\n\n  # Invoke\n  messages = [HumanMessage(content=\"Add 3 and 4.\")]\n  for chunk in agent.stream(messages, stream_mode=\"updates\"):\n      print(chunk)\n      print(\"\\n\")\n  ```\n</CodeGroup>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Agents are LLM-powered systems that autonomously use tools in feedback loops to solve unpredictable problems, with more decision-making freedom than workflows.", "keywords": ["LLM", "tools", "feedback loops", "autonomy", "LangGraph", "StateGraph", "tool_calls", "conditional edges", "MessagesState", "bind_tools"]}