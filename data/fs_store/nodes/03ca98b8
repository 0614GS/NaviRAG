{"node_id": "03ca98b8", "title": "Runtime context", "path": "graph-api > Graph API overview > Runtime context", "content": "When creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\n\n```python  theme={null}\n@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n```\n\nYou can then pass this context into the graph using the `context` parameter of the `invoke` method.\n\n```python  theme={null}\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n```\n\nYou can then access and use this context inside a node or conditional edge:\n\n```python  theme={null}\nfrom langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    # ...\n```\n\nSee [this guide](/oss/python/langgraph/use-graph-api#add-runtime-configuration) for a full breakdown on configuration.", "summary": "Explains how to specify and use runtime context (like `llm_provider`) in a LangGraph graph via `context_schema`, and covers recursion limits and counter handling for graph execution control.", "keywords": ["context_schema", "Runtime", "invoke", "recursion_limit", "GraphRecursionError", "langgraph_step", "RemainingSteps", "StateGraph", "RunnableConfig", "super-steps"]}