{"node_id": "7026c43a", "title": "4. Grade documents", "path": "agentic-rag > Build a custom RAG agent with LangGraph > 4. Grade documents", "content": "1. Add a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) — `grade_documents` — to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schema `GradeDocuments` for document grading. The `grade_documents` function will return the name of the node to go to based on the grading decision (`generate_answer` or `rewrite_question`):\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\nfrom typing import Literal\n\nGRADE_PROMPT = (\n    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n    \"Here is the user question: {question} \\n\"\n    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n)\n\n\nclass GradeDocuments(BaseModel):  # [!code highlight]\n    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n    )\n\n\ngrader_model = init_chat_model(\"gpt-4o\", temperature=0)\n\n\ndef grade_documents(\n    state: MessagesState,\n) -> Literal[\"generate_answer\", \"rewrite_question\"]:\n    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n\n    prompt = GRADE_PROMPT.format(question=question, context=context)\n    response = (\n        grader_model\n        .with_structured_output(GradeDocuments).invoke(  # [!code highlight]\n            [{\"role\": \"user\", \"content\": prompt}]\n        )\n    )\n    score = response.binary_score\n\n    if score == \"yes\":\n        return \"generate_answer\"\n    else:\n        return \"rewrite_question\"\n```\n\n2. Run this with irrelevant documents in the tool response:\n\n```python  theme={null}\nfrom langchain_core.messages import convert_to_messages\n\ninput = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\ngrade_documents(input)\n```\n\n3. Confirm that the relevant documents are classified as such:\n\n```python  theme={null}\ninput = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\ngrade_documents(input)\n```", "summary": "Implements a conditional edge in LangGraph to grade document relevance using a structured LLM, routing to answer generation or question rewriting.", "keywords": ["GradeDocuments", "conditional edge", "structured output", "binary_score", "grade_documents", "LangGraph", "relevance grading", "GPT-4o", "GRADE_PROMPT", "rewrite_question"]}