{"node_id": "3c4b07d5", "title": "Summarize messages", "path": "add-memory > Memory > Manage short-term memory > Summarize messages", "content": "The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=c8ed3facdccd4ef5c7e52902c72ba938\" alt=\"Summary\" data-og-width=\"609\" width=\"609\" data-og-height=\"242\" height=\"242\" data-path=\"oss/images/summary.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=280&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4208b9b0cc9f459f3dc4e5219918471b 280w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=560&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=7acb77c081545f57042368f4e9d0c8cb 560w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=840&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=2fcfdb0c481d2e1d361e76db763a41e5 840w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1100&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=4abdac693a562788aa0db8681bef8ea7 1100w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=1650&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=40acfefa91dcb11b247a6e4a7705f22b 1650w, https://mintcdn.com/langchain-5e9cc07a/ybiAaBfoBvFquMDz/oss/images/summary.png?w=2500&fit=max&auto=format&n=ybiAaBfoBvFquMDz&q=85&s=8d765aaf7551e8b0fc2720de7d2ac2a8 2500w\" />\n\nPrompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the [`MessagesState`](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) to include a `summary` key:\n\n```python  theme={null}\nfrom langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n```\n\nThen, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize_conversation` node can be called after some number of messages have accumulated in the `messages` state key.\n\n```python  theme={null}\ndef summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n```\n\n<Accordion title=\"Full example: summarize messages\">\n  ```python  theme={null}\n  from typing import Any, TypedDict\n\n  from langchain.chat_models import init_chat_model\n  from langchain.messages import AnyMessage\n  from langchain_core.messages.utils import count_tokens_approximately\n  from langgraph.graph import StateGraph, START, MessagesState\n  from langgraph.checkpoint.memory import InMemorySaver\n  from langmem.short_term import SummarizationNode, RunningSummary  # [!code highlight]\n\n  model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\n  class State(MessagesState):\n      context: dict[str, RunningSummary]  # [!code highlight]\n\n  class LLMInputState(TypedDict):  # [!code highlight]\n      summarized_messages: list[AnyMessage]\n      context: dict[str, RunningSummary]\n\n  summarization_node = SummarizationNode(  # [!code highlight]\n      token_counter=count_tokens_approximately,\n      model=summarization_model,\n      max_tokens=256,\n      max_tokens_before_summary=256,\n      max_summary_tokens=128,\n  )\n\n  def call_model(state: LLMInputState):  # [!code highlight]\n      response = model.invoke(state[\"summarized_messages\"])\n      return {\"messages\": [response]}\n\n  checkpointer = InMemorySaver()\n  builder = StateGraph(State)\n  builder.add_node(call_model)\n  builder.add_node(\"summarize\", summarization_node)  # [!code highlight]\n  builder.add_edge(START, \"summarize\")\n  builder.add_edge(\"summarize\", \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\n  # Invoke the graph\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\n  final_response[\"messages\"][-1].pretty_print()\n  print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n  ```\n\n  1. We will keep track of our running summary in the `context` field\n\n  (expected by the `SummarizationNode`).\n\n  1. Define private state that will be used only for filtering\n\n  the inputs to `call_model` node.\n\n  1. We're passing a private input state here to isolate the messages returned by the summarization node\n\n  ```\n  ================================== Ai Message ==================================\n\n  From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\n  Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n  ```\n</Accordion>", "summary": "Describes using a chat model to summarize message history as a sophisticated alternative to trimming, preventing information loss in applications.", "keywords": ["Summarize messages", "chat model", "message history", "LangGraph", "MessagesState", "summary key", "summarize_conversation node", "RunningSummary", "SummarizationNode", "token counter"]}