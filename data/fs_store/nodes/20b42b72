{"node_id": "20b42b72", "title": "Filter by LLM invocation", "path": "streaming > Streaming > LLM tokens > Filter by LLM invocation", "content": "You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation.\n\n```python  theme={null}\nfrom langchain.chat_models import init_chat_model\n\n# model_1 is tagged with \"joke\"\nmodel_1 = init_chat_model(model=\"gpt-4o-mini\", tags=['joke'])\n# model_2 is tagged with \"poem\"\nmodel_2 = init_chat_model(model=\"gpt-4o-mini\", tags=['poem'])\n\ngraph = ... # define a graph that uses these LLMs\n\n# The stream_mode is set to \"messages\" to stream LLM tokens\n# The metadata contains information about the LLM invocation, including the tags\nasync for msg, metadata in graph.astream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    # Filter the streamed tokens by the tags field in the metadata to only include\n    # the tokens from the LLM invocation with the \"joke\" tag\n    if metadata[\"tags\"] == [\"joke\"]:\n        print(msg.content, end=\"|\", flush=True)\n```\n\n<Accordion title=\"Extended example: filtering by tags\">\n  ```python  theme={null}\n  from typing import TypedDict\n\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import START, StateGraph\n\n  # The joke_model is tagged with \"joke\"\n  joke_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"joke\"])\n  # The poem_model is tagged with \"poem\"\n  poem_model = init_chat_model(model=\"gpt-4o-mini\", tags=[\"poem\"])\n\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n        poem: str\n\n\n  async def call_model(state, config):\n        topic = state[\"topic\"]\n        print(\"Writing joke...\")\n        # Note: Passing the config through explicitly is required for python < 3.11\n        # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n        # The config is passed through explicitly to ensure the context vars are propagated correctly\n        # This is required for Python < 3.11 when using async code. Please see the async section for more details\n        joke_response = await joke_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n              config,\n        )\n        print(\"\\n\\nWriting poem...\")\n        poem_response = await poem_model.ainvoke(\n              [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n              config,\n        )\n        return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\n\n  graph = (\n        StateGraph(State)\n        .add_node(call_model)\n        .add_edge(START, \"call_model\")\n        .compile()\n  )\n\n  # The stream_mode is set to \"messages\" to stream LLM tokens\n  # The metadata contains information about the LLM invocation, including the tags\n  async for msg, metadata in graph.astream(\n        {\"topic\": \"cats\"},\n        stream_mode=\"messages\",\n  ):\n      if metadata[\"tags\"] == [\"joke\"]:\n          print(msg.content, end=\"|\", flush=True)\n  ```\n</Accordion>", "summary": "Explains how to filter streamed LLM tokens by associating tags with LLM invocations and using metadata in LangGraph.", "keywords": ["tags", "LLM invocation", "metadata", "astream", "stream_mode", "init_chat_model", "StateGraph", "LangGraph", "filter", "tokens"]}