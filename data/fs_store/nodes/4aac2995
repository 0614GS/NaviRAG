{"node_id": "4aac2995", "title": "Add runtime configuration", "path": "use-graph-api > Use the graph API > Add runtime configuration", "content": "Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, *without polluting the graph state with these parameters*.\n\nTo add runtime configuration:\n\n1. Specify a schema for your configuration\n2. Add the configuration to the function signature for nodes or conditional edges\n3. Pass the configuration into the graph.\n\nSee below for a simple example:\n\n```python  theme={null}\nfrom langgraph.graph import END, StateGraph, START\nfrom langgraph.runtime import Runtime\nfrom typing_extensions import TypedDict\n\n# 1. Specify config schema\nclass ContextSchema(TypedDict):\n    my_runtime_value: str\n\n# 2. Define a graph that accesses the config in a node\nclass State(TypedDict):\n    my_state_value: str\n\ndef node(state: State, runtime: Runtime[ContextSchema]):  # [!code highlight]\n    if runtime.context[\"my_runtime_value\"] == \"a\":  # [!code highlight]\n        return {\"my_state_value\": 1}\n    elif runtime.context[\"my_runtime_value\"] == \"b\":  # [!code highlight]\n        return {\"my_state_value\": 2}\n    else:\n        raise ValueError(\"Unknown values.\")\n\nbuilder = StateGraph(State, context_schema=ContextSchema)  # [!code highlight]\nbuilder.add_node(node)\nbuilder.add_edge(START, \"node\")\nbuilder.add_edge(\"node\", END)\n\ngraph = builder.compile()\n\n# 3. Pass in configuration at runtime:\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))  # [!code highlight]\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))  # [!code highlight]\n```\n\n```\n{'my_state_value': 1}\n{'my_state_value': 2}\n```\n\n<Accordion title=\"Extended example: specifying LLM at runtime\">\n  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models.\n\n  ```python  theme={null}\n  from dataclasses import dataclass\n\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import MessagesState, END, StateGraph, START\n  from langgraph.runtime import Runtime\n  from typing_extensions import TypedDict\n\n  @dataclass\n  class ContextSchema:\n      model_provider: str = \"anthropic\"\n\n  MODELS = {\n      \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\n      \"openai\": init_chat_model(\"gpt-4.1-mini\"),\n  }\n\n  def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n      model = MODELS[runtime.context.model_provider]\n      response = model.invoke(state[\"messages\"])\n      return {\"messages\": [response]}\n\n  builder = StateGraph(MessagesState, context_schema=ContextSchema)\n  builder.add_node(\"model\", call_model)\n  builder.add_edge(START, \"model\")\n  builder.add_edge(\"model\", END)\n\n  graph = builder.compile()\n\n  # Usage\n  input_message = {\"role\": \"user\", \"content\": \"hi\"}\n  # With no configuration, uses default (Anthropic)\n  response_1 = graph.invoke({\"messages\": [input_message]}, context=ContextSchema())[\"messages\"][-1]\n  # Or, can set OpenAI\n  response_2 = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\"})[\"messages\"][-1]\n\n  print(response_1.response_metadata[\"model_name\"])\n  print(response_2.response_metadata[\"model_name\"])\n  ```\n\n  ```\n  claude-haiku-4-5-20251001\n  gpt-4.1-mini-2025-04-14\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: specifying model and system message at runtime\">\n  Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime.\n\n  ```python  theme={null}\n  from dataclasses import dataclass\n  from langchain.chat_models import init_chat_model\n  from langchain.messages import SystemMessage\n  from langgraph.graph import END, MessagesState, StateGraph, START\n  from langgraph.runtime import Runtime\n  from typing_extensions import TypedDict\n\n  @dataclass\n  class ContextSchema:\n      model_provider: str = \"anthropic\"\n      system_message: str | None = None\n\n  MODELS = {\n      \"anthropic\": init_chat_model(\"claude-haiku-4-5-20251001\"),\n      \"openai\": init_chat_model(\"gpt-4.1-mini\"),\n  }\n\n  def call_model(state: MessagesState, runtime: Runtime[ContextSchema]):\n      model = MODELS[runtime.context.model_provider]\n      messages = state[\"messages\"]\n      if (system_message := runtime.context.system_message):\n          messages = [SystemMessage(system_message)] + messages\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  builder = StateGraph(MessagesState, context_schema=ContextSchema)\n  builder.add_node(\"model\", call_model)\n  builder.add_edge(START, \"model\")\n  builder.add_edge(\"model\", END)\n\n  graph = builder.compile()\n\n  # Usage\n  input_message = {\"role\": \"user\", \"content\": \"hi\"}\n  response = graph.invoke({\"messages\": [input_message]}, context={\"model_provider\": \"openai\", \"system_message\": \"Respond in Italian.\"})\n  for message in response[\"messages\"]:\n      message.pretty_print()\n  ```\n\n  ```\n  ================================ Human Message ================================\n\n  hi\n  ================================== Ai Message ==================================\n\n  Ciao! Come posso aiutarti oggi?\n  ```\n</Accordion>", "summary": "Explains how to add runtime configuration to LangGraph graphs, allowing dynamic specification of parameters like LLM models and system prompts without modifying graph state.", "keywords": ["Runtime", "context_schema", "StateGraph", "invoke", "TypedDict", "MessagesState", "init_chat_model", "SystemMessage", "model_provider", "system_message"]}