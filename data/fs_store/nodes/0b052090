{"node_id": "0b052090", "title": "1. Select an LLM", "path": "sql-agent > Build a custom SQL agent > 1. Select an LLM", "content": "Select a model that supports [tool-calling](/oss/python/integrations/providers/overview):\n\n<Tabs>\n  <Tab title=\"OpenAI\">\n    ðŸ‘‰ Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"gpt-4.1\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import ChatOpenAI\n\n      os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n      model = ChatOpenAI(model=\"gpt-4.1\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Anthropic\">\n    ðŸ‘‰ Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[anthropic]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_anthropic import ChatAnthropic\n\n      os.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\n      model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Azure\">\n    ðŸ‘‰ Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure_chat_openai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[openai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = init_chat_model(\n          \"azure_openai:gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_openai import AzureChatOpenAI\n\n      os.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\n      os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\n      os.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\n      model = AzureChatOpenAI(\n          model=\"gpt-4.1\",\n          azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"]\n      )\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"Google Gemini\">\n    ðŸ‘‰ Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google_generative_ai/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[google-genai]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_google_genai import ChatGoogleGenerativeAI\n\n      os.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\n      model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"AWS Bedrock\">\n    ðŸ‘‰ Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[aws]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      from langchain.chat_models import init_chat_model\n\n      # Follow the steps here to configure your credentials:\n      # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\n      model = init_chat_model(\n          \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n          model_provider=\"bedrock_converse\",\n      )\n      ```\n\n      ```python Model Class theme={null}\n      from langchain_aws import ChatBedrock\n\n      model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\")\n      ```\n    </CodeGroup>\n  </Tab>\n\n  <Tab title=\"HuggingFace\">\n    ðŸ‘‰ Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/)\n\n    ```shell  theme={null}\n    pip install -U \"langchain[huggingface]\"\n    ```\n\n    <CodeGroup>\n      ```python init_chat_model theme={null}\n      import os\n      from langchain.chat_models import init_chat_model\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      model = init_chat_model(\n          \"microsoft/Phi-3-mini-4k-instruct\",\n          model_provider=\"huggingface\",\n          temperature=0.7,\n          max_tokens=1024,\n      )\n      ```\n\n      ```python Model Class theme={null}\n      import os\n      from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n\n      os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_...\"\n\n      llm = HuggingFaceEndpoint(\n          repo_id=\"microsoft/Phi-3-mini-4k-instruct\",\n          temperature=0.7,\n          max_length=1024,\n      )\n      model = ChatHuggingFace(llm=llm)\n      ```\n    </CodeGroup>\n  </Tab>\n</Tabs>\n\nThe output shown in the examples below used OpenAI.", "summary": "Instructions for selecting and initializing LLMs with tool-calling support from various providers (OpenAI, Anthropic, Azure, Google Gemini, AWS Bedrock, HuggingFace) using LangChain.", "keywords": ["init_chat_model", "ChatOpenAI", "ChatAnthropic", "AzureChatOpenAI", "ChatGoogleGenerativeAI", "ChatBedrock", "ChatHuggingFace", "tool-calling", "LangChain", "API key"]}