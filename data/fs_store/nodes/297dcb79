{"node_id": "297dcb79", "title": "Caching Tasks", "path": "use-functional-api > Use the functional API > Caching Tasks", "content": "```python  theme={null}\nimport time\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import CachePolicy\n\n\n@task(cache_policy=CachePolicy(ttl=120))    # [!code highlight]\ndef slow_add(x: int) -> int:\n    time.sleep(1)\n    return x * 2\n\n\n@entrypoint(cache=InMemoryCache())\ndef main(inputs: dict) -> dict[str, int]:\n    result1 = slow_add(inputs[\"x\"]).result()\n    result2 = slow_add(inputs[\"x\"]).result()\n    return {\"result1\": result1, \"result2\": result2}\n\n\nfor chunk in main.stream({\"x\": 5}, stream_mode=\"updates\"):\n    print(chunk)\n\n#> {'slow_add': 10}\n#> {'slow_add': 10, '__metadata__': {'cached': True}}\n#> {'main': {'result1': 10, 'result2': 10}}\n```\n\n1. `ttl` is specified in seconds. The cache will be invalidated after this time.", "summary": "Demonstrates caching tasks in LangGraph using TTL and InMemoryCache to avoid redundant computations.", "keywords": ["CachePolicy", "ttl", "InMemoryCache", "task", "entrypoint", "caching", "slow_add", "stream", "updates", "cached"]}