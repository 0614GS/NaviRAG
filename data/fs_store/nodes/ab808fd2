{"node_id": "ab808fd2", "title": "8. Run the agentic RAG", "path": "agentic-rag > Build a custom RAG agent with LangGraph > 8. Run the agentic RAG", "content": "Now let's test the complete graph by running it with a question:\n\n```python  theme={null}\nfor chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            }\n        ]\n    }\n):\n    for node, update in chunk.items():\n        print(\"Update from node\", node)\n        update[\"messages\"][-1].pretty_print()\n        print(\"\\n\\n\")\n```\n\n**Output:**\n\n```\nUpdate from node generate_query_or_respond\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_blog_posts (call_NYu2vq4km9nNNEFqJwefWKu1)\n Call ID: call_NYu2vq4km9nNNEFqJwefWKu1\n  Args:\n    query: types of reward hacking\n\n\n\nUpdate from node retrieve\n================================= Tool Message ==================================\nName: retrieve_blog_posts\n\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n\nWhy does Reward Hacking Exist?#\n\nPan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:\n\nLet's Define Reward Hacking#\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\n\n\n\nUpdate from node generate_answer\n================================== Ai Message ==================================\n\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/agentic-rag.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Demonstrates running a LangGraph agentic RAG system with a query about reward hacking, showing the step-by-step node outputs and final answer.", "keywords": ["LangGraph", "agentic RAG", "graph.stream", "retrieve_blog_posts", "generate_query_or_respond", "generate_answer", "reward hacking", "reward tampering", "environment misspecification", "goal misspecification"]}