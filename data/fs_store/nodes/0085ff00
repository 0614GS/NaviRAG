{"node_id": "0085ff00", "title": "Parallel execution", "path": "use-functional-api > Use the functional API > Parallel execution", "content": "Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).\n\n```python  theme={null}\n@task\ndef add_one(number: int) -> int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -> list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n```\n\n<Accordion title=\"Extended example: parallel LLM calls\">\n  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output.\n\n  ```python  theme={null}\n  import uuid\n  from langchain.chat_models import init_chat_model\n  from langgraph.func import entrypoint, task\n  from langgraph.checkpoint.memory import InMemorySaver\n\n  # Initialize the LLM model\n  model = init_chat_model(\"gpt-3.5-turbo\")\n\n  # Task that generates a paragraph about a given topic\n  @task\n  def generate_paragraph(topic: str) -> str:\n      response = model.invoke([\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n          {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n      ])\n      return response.content\n\n  # Create a checkpointer for persistence\n  checkpointer = InMemorySaver()\n\n  @entrypoint(checkpointer=checkpointer)\n  def workflow(topics: list[str]) -> str:\n      \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n      futures = [generate_paragraph(topic) for topic in topics]\n      paragraphs = [f.result() for f in futures]\n      return \"\\n\\n\".join(paragraphs)\n\n  # Run the workflow\n  config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n  result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\n  print(result)\n  ```\n\n  This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.\n</Accordion>", "summary": "Explains how to execute tasks concurrently in LangGraph using the functional API, with examples for parallel LLM calls to improve performance in IO-bound operations.", "keywords": ["@task", "@entrypoint", "parallel execution", "futures", "concurrency", "IO bound tasks", "LLM calls", "InMemorySaver", "checkpointer", "workflow"]}