{"node_id": "a574bd96", "title": "Use with any LLM", "path": "streaming > Streaming > Use with any LLM", "content": "You can use `stream_mode=\"custom\"` to stream data from **any LLM API** â€” even if that API does **not** implement the LangChain chat model interface.\n\nThis lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.\n\n```python  theme={null}\nfrom langgraph.config import get_stream_writer\n\ndef call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    # Get the stream writer to send custom data\n    writer = get_stream_writer()  # [!code highlight]\n    # Assume you have a streaming client that yields chunks\n    # Generate LLM tokens using your custom streaming client\n    for chunk in your_custom_streaming_client(state[\"topic\"]):\n        # Use the writer to send custom data to the stream\n        writer({\"custom_llm_chunk\": chunk})  # [!code highlight]\n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n# Set stream_mode=\"custom\" to receive the custom data in the stream\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    stream_mode=\"custom\",  # [!code highlight]\n\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n```\n\n<Accordion title=\"Extended example: streaming arbitrary chat model\">\n  ```python  theme={null}\n  import operator\n  import json\n\n  from typing import TypedDict\n  from typing_extensions import Annotated\n  from langgraph.graph import StateGraph, START\n\n  from openai import AsyncOpenAI\n\n  openai_client = AsyncOpenAI()\n  model_name = \"gpt-4o-mini\"\n\n\n  async def stream_tokens(model_name: str, messages: list[dict]):\n      response = await openai_client.chat.completions.create(\n          messages=messages, model=model_name, stream=True\n      )\n      role = None\n      async for chunk in response:\n          delta = chunk.choices[0].delta\n\n          if delta.role is not None:\n              role = delta.role\n\n          if delta.content:\n              yield {\"role\": role, \"content\": delta.content}\n\n\n  # this is our tool\n  async def get_items(place: str) -> str:\n      \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n      writer = get_stream_writer()\n      response = \"\"\n      async for msg_chunk in stream_tokens(\n          model_name,\n          [\n              {\n                  \"role\": \"user\",\n                  \"content\": (\n                      \"Can you tell me what kind of items \"\n                      f\"i might find in the following place: '{place}'. \"\n                      \"List at least 3 such items separating them by a comma. \"\n                      \"And include a brief description of each item.\"\n                  ),\n              }\n          ],\n      ):\n          response += msg_chunk[\"content\"]\n          writer(msg_chunk)\n\n      return response\n\n\n  class State(TypedDict):\n      messages: Annotated[list[dict], operator.add]\n\n\n  # this is the tool-calling graph node\n  async def call_tool(state: State):\n      ai_message = state[\"messages\"][-1]\n      tool_call = ai_message[\"tool_calls\"][-1]\n\n      function_name = tool_call[\"function\"][\"name\"]\n      if function_name != \"get_items\":\n          raise ValueError(f\"Tool {function_name} not supported\")\n\n      function_arguments = tool_call[\"function\"][\"arguments\"]\n      arguments = json.loads(function_arguments)\n\n      function_response = await get_items(**arguments)\n      tool_message = {\n          \"tool_call_id\": tool_call[\"id\"],\n          \"role\": \"tool\",\n          \"name\": function_name,\n          \"content\": function_response,\n      }\n      return {\"messages\": [tool_message]}\n\n\n  graph = (\n      StateGraph(State)\n      .add_node(call_tool)\n      .add_edge(START, \"call_tool\")\n      .compile()\n  )\n  ```\n\n  Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call:\n\n  ```python  theme={null}\n  inputs = {\n      \"messages\": [\n          {\n              \"content\": None,\n              \"role\": \"assistant\",\n              \"tool_calls\": [\n                  {\n                      \"id\": \"1\",\n                      \"function\": {\n                          \"arguments\": '{\"place\":\"bedroom\"}',\n                          \"name\": \"get_items\",\n                      },\n                      \"type\": \"function\",\n                  }\n              ],\n          }\n      ]\n  }\n\n  async for chunk in graph.astream(\n      inputs,\n      stream_mode=\"custom\",\n  ):\n      print(chunk[\"content\"], end=\"|\", flush=True)\n  ```\n</Accordion>", "summary": "Enables streaming from any LLM API using custom mode, integrating external clients and services for flexible workflows.", "keywords": ["stream_mode=\"custom\"", "get_stream_writer", "custom_llm_chunk", "StateGraph", "stream_tokens", "AsyncOpenAI", "tool_calling", "astream", "LangGraph", "arbitrary model"]}