{"node_id": "375803fd", "title": "Async with Python \\< 3.11", "path": "streaming > Streaming > Disable streaming for specific chat models > Async with Python \\< 3.11", "content": "In Python versions \\< 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task) do not support the `context` parameter.\nThis limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:\n\n1. You **must** explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated.\n2. You **cannot** use [`get_stream_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get_stream_writer) in async nodes or tools â€” you must pass a `writer` argument directly.\n\n<Accordion title=\"Extended example: async LLM call with manual config\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import START, StateGraph\n  from langchain.chat_models import init_chat_model\n\n  model = init_chat_model(model=\"gpt-4o-mini\")\n\n  class State(TypedDict):\n      topic: str\n      joke: str\n\n  # Accept config as an argument in the async node function\n  async def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Generating joke...\")\n      # Pass config to model.ainvoke() to ensure proper context propagation\n      joke_response = await model.ainvoke(  # [!code highlight]\n          [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n          config,\n      )\n      return {\"joke\": joke_response.content}\n\n  graph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n  )\n\n  # Set stream_mode=\"messages\" to stream LLM tokens\n  async for chunk, metadata in graph.astream(\n      {\"topic\": \"ice cream\"},\n      stream_mode=\"messages\",  # [!code highlight]\n  ):\n      if chunk.content:\n          print(chunk.content, end=\"|\", flush=True)\n  ```\n</Accordion>\n\n<Accordion title=\"Extended example: async custom streaming with stream writer\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.types import StreamWriter\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n\n  # Add writer as an argument in the function signature of the async node or tool\n  # LangGraph will automatically pass the stream writer to the function\n  async def generate_joke(state: State, writer: StreamWriter):  # [!code highlight]\n        writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n        return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\n  graph = (\n        StateGraph(State)\n        .add_node(generate_joke)\n        .add_edge(START, \"generate_joke\")\n        .compile()\n  )\n\n  # Set stream_mode=\"custom\" to receive the custom data in the stream  # [!code highlight]\n  async for chunk in graph.astream(\n        {\"topic\": \"ice cream\"},\n        stream_mode=\"custom\",\n  ):\n        print(chunk)\n  ```\n</Accordion>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Limitations and workarounds for async streaming in LangGraph with Python < 3.11, including manual RunnableConfig passing and direct writer usage.", "keywords": ["asyncio", "RunnableConfig", "ainvoke", "get_stream_writer", "StreamWriter", "async nodes", "context propagation", "stream_mode", "LangGraph", "Python < 3.11"]}