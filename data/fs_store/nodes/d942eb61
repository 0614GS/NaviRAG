{"node_id": "d942eb61", "title": "Prompt chaining", "path": "workflows-agents > Workflows and agents > Prompt chaining", "content": "Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n\n* Translating documents into different languages\n* Verifying generated content for consistency\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=762dec147c31b8dc6ebb0857e236fc1f\" alt=\"Prompt chaining\" data-og-width=\"1412\" width=\"1412\" data-og-height=\"444\" height=\"444\" data-path=\"oss/images/prompt_chain.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=fda27cf4f997e350d4ce48be16049c47 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=1374b6de11900d394fc73722a3a6040e 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=25246c7111a87b5df5a2af24a0181efe 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=0c57da86a49cf966cc090497ade347f1 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=a1b5c8fc644d7a80c0792b71769c97da 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/prompt_chain.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8a3f66f0e365e503a85b30be48bc1a76 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  from typing_extensions import TypedDict\n  from langgraph.graph import StateGraph, START, END\n  from IPython.display import Image, display\n\n\n  # Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      improved_joke: str\n      final_joke: str\n\n\n  # Nodes\n  def generate_joke(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\n      msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\n\n  def check_punchline(state: State):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n      # Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n          return \"Pass\"\n      return \"Fail\"\n\n\n  def improve_joke(state: State):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n\n      msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n      return {\"improved_joke\": msg.content}\n\n\n  def polish_joke(state: State):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n      return {\"final_joke\": msg.content}\n\n\n  # Build workflow\n  workflow = StateGraph(State)\n\n  # Add nodes\n  workflow.add_node(\"generate_joke\", generate_joke)\n  workflow.add_node(\"improve_joke\", improve_joke)\n  workflow.add_node(\"polish_joke\", polish_joke)\n\n  # Add edges to connect nodes\n  workflow.add_edge(START, \"generate_joke\")\n  workflow.add_conditional_edges(\n      \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n  )\n  workflow.add_edge(\"improve_joke\", \"polish_joke\")\n  workflow.add_edge(\"polish_joke\", END)\n\n  # Compile\n  chain = workflow.compile()\n\n  # Show workflow\n  display(Image(chain.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = chain.invoke({\"topic\": \"cats\"})\n  print(\"Initial joke:\")\n  print(state[\"joke\"])\n  print(\"\\n--- --- ---\\n\")\n  if \"improved_joke\" in state:\n      print(\"Improved joke:\")\n      print(state[\"improved_joke\"])\n      print(\"\\n--- --- ---\\n\")\n\n      print(\"Final joke:\")\n      print(state[\"final_joke\"])\n  else:\n      print(\"Final joke:\")\n      print(state[\"joke\"])\n  ```\n\n  ```python Functional API theme={null}\n  from langgraph.func import entrypoint, task\n\n\n  # Tasks\n  @task\n  def generate_joke(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a short joke about {topic}\")\n      return msg.content\n\n\n  def check_punchline(joke: str):\n      \"\"\"Gate function to check if the joke has a punchline\"\"\"\n      # Simple check - does the joke contain \"?\" or \"!\"\n      if \"?\" in joke or \"!\" in joke:\n          return \"Fail\"\n\n      return \"Pass\"\n\n\n  @task\n  def improve_joke(joke: str):\n      \"\"\"Second LLM call to improve the joke\"\"\"\n      msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n      return msg.content\n\n\n  @task\n  def polish_joke(joke: str):\n      \"\"\"Third LLM call for final polish\"\"\"\n      msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n      return msg.content\n\n\n  @entrypoint()\n  def prompt_chaining_workflow(topic: str):\n      original_joke = generate_joke(topic).result()\n      if check_punchline(original_joke) == \"Pass\":\n          return original_joke\n\n      improved_joke = improve_joke(original_joke).result()\n      return polish_joke(improved_joke).result()\n\n  # Invoke\n  for step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>", "summary": "Prompt chaining involves sequential LLM calls where each processes the previous output, used for tasks like translation and content verification.", "keywords": ["LLM", "Prompt chaining", "StateGraph", "TypedDict", "conditional edges", "gate function", "workflow", "LangGraph", "Graph API", "Functional API"]}