{"node_id": "8803a93c", "title": "Parallelization", "path": "workflows-agents > Workflows and agents > Parallelization", "content": "With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n\n* Split up subtasks and run them in parallel, which increases speed\n* Run tasks multiple times to check for different outputs, which increases confidence\n\nSome examples include:\n\n* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n\n<img src=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=8afe3c427d8cede6fed1e4b2a5107b71\" alt=\"parallelization.png\" data-og-width=\"1020\" width=\"1020\" data-og-height=\"684\" height=\"684\" data-path=\"oss/images/parallelization.png\" data-optimize=\"true\" data-opv=\"3\" srcset=\"https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=280&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=88e51062b14d9186a6f0ea246bc48635 280w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=560&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=934941ca52019b7cbce7fbdd31d00f0f 560w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=840&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=30b5c86c545d0e34878ff0a2c367dd0a 840w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1100&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=6227d2c39f332eaeda23f7db66871dd7 1100w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=1650&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=283f3ee2924a385ab88f2cbfd9c9c48c 1650w, https://mintcdn.com/langchain-5e9cc07a/dL5Sn6Cmy9pwtY0V/oss/images/parallelization.png?w=2500&fit=max&auto=format&n=dL5Sn6Cmy9pwtY0V&q=85&s=69f6a97716b38998b7b399c3d8ac7d9c 2500w\" />\n\n<CodeGroup>\n  ```python Graph API theme={null}\n  # Graph state\n  class State(TypedDict):\n      topic: str\n      joke: str\n      story: str\n      poem: str\n      combined_output: str\n\n\n  # Nodes\n  def call_llm_1(state: State):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n\n      msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n      return {\"joke\": msg.content}\n\n\n  def call_llm_2(state: State):\n      \"\"\"Second LLM call to generate story\"\"\"\n\n      msg = llm.invoke(f\"Write a story about {state['topic']}\")\n      return {\"story\": msg.content}\n\n\n  def call_llm_3(state: State):\n      \"\"\"Third LLM call to generate poem\"\"\"\n\n      msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n      return {\"poem\": msg.content}\n\n\n  def aggregator(state: State):\n      \"\"\"Combine the joke, story and poem into a single output\"\"\"\n\n      combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n      combined += f\"STORY:\\n{state['story']}\\n\\n\"\n      combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n      combined += f\"POEM:\\n{state['poem']}\"\n      return {\"combined_output\": combined}\n\n\n  # Build workflow\n  parallel_builder = StateGraph(State)\n\n  # Add nodes\n  parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n  parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n  parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n  parallel_builder.add_node(\"aggregator\", aggregator)\n\n  # Add edges to connect nodes\n  parallel_builder.add_edge(START, \"call_llm_1\")\n  parallel_builder.add_edge(START, \"call_llm_2\")\n  parallel_builder.add_edge(START, \"call_llm_3\")\n  parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n  parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n  parallel_builder.add_edge(\"aggregator\", END)\n  parallel_workflow = parallel_builder.compile()\n\n  # Show workflow\n  display(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n  # Invoke\n  state = parallel_workflow.invoke({\"topic\": \"cats\"})\n  print(state[\"combined_output\"])\n  ```\n\n  ```python Functional API theme={null}\n  @task\n  def call_llm_1(topic: str):\n      \"\"\"First LLM call to generate initial joke\"\"\"\n      msg = llm.invoke(f\"Write a joke about {topic}\")\n      return msg.content\n\n\n  @task\n  def call_llm_2(topic: str):\n      \"\"\"Second LLM call to generate story\"\"\"\n      msg = llm.invoke(f\"Write a story about {topic}\")\n      return msg.content\n\n\n  @task\n  def call_llm_3(topic):\n      \"\"\"Third LLM call to generate poem\"\"\"\n      msg = llm.invoke(f\"Write a poem about {topic}\")\n      return msg.content\n\n\n  @task\n  def aggregator(topic, joke, story, poem):\n      \"\"\"Combine the joke and story into a single output\"\"\"\n\n      combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n      combined += f\"STORY:\\n{story}\\n\\n\"\n      combined += f\"JOKE:\\n{joke}\\n\\n\"\n      combined += f\"POEM:\\n{poem}\"\n      return combined\n\n\n  # Build workflow\n  @entrypoint()\n  def parallel_workflow(topic: str):\n      joke_fut = call_llm_1(topic)\n      story_fut = call_llm_2(topic)\n      poem_fut = call_llm_3(topic)\n      return aggregator(\n          topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n      ).result()\n\n  # Invoke\n  for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n      print(step)\n      print(\"\\n\")\n  ```\n</CodeGroup>", "summary": "Explains parallelization in LLM workflows, where tasks are split into independent subtasks or run multiple times to increase speed and confidence, with code examples.", "keywords": ["Parallelization", "LLM", "StateGraph", "TypedDict", "aggregator", "subtasks", "workflow", "Graph API", "Functional API", "call_llm"]}