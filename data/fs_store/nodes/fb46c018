{"node_id": "fb46c018", "title": "Trim messages", "path": "add-memory > Memory > Manage short-term memory > Trim messages", "content": "Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max_tokens`) to use for handling the boundary.\n\nTo trim message history, use the [`trim_messages`](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.utils.trim_messages.html) function:\n\n```python  theme={null}\nfrom langchain_core.messages.utils import (  # [!code highlight]\n    trim_messages,  # [!code highlight]\n    count_tokens_approximately  # [!code highlight]\n)  # [!code highlight]\n\ndef call_model(state: MessagesState):\n    messages = trim_messages(  # [!code highlight]\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n```\n\n<Accordion title=\"Full example: trim messages\">\n  ```python  theme={null}\n  from langchain_core.messages.utils import (\n      trim_messages,  # [!code highlight]\n      count_tokens_approximately  # [!code highlight]\n  )\n  from langchain.chat_models import init_chat_model\n  from langgraph.graph import StateGraph, START, MessagesState\n\n  model = init_chat_model(\"claude-sonnet-4-5-20250929\")\n  summarization_model = model.bind(max_tokens=128)\n\n  def call_model(state: MessagesState):\n      messages = trim_messages(  # [!code highlight]\n          state[\"messages\"],\n          strategy=\"last\",\n          token_counter=count_tokens_approximately,\n          max_tokens=128,\n          start_on=\"human\",\n          end_on=(\"human\", \"tool\"),\n      )\n      response = model.invoke(messages)\n      return {\"messages\": [response]}\n\n  checkpointer = InMemorySaver()\n  builder = StateGraph(MessagesState)\n  builder.add_node(call_model)\n  builder.add_edge(START, \"call_model\")\n  graph = builder.compile(checkpointer=checkpointer)\n\n  config = {\"configurable\": {\"thread_id\": \"1\"}}\n  graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n  graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n  graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n  final_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\n  final_response[\"messages\"][-1].pretty_print()\n  ```\n\n  ```\n  ================================== Ai Message ==================================\n\n  Your name is Bob, as you mentioned when you first introduced yourself.\n  ```\n</Accordion>", "summary": "Describes how to use the `trim_messages` utility in LangChain to truncate message history based on token count to fit within an LLM's context window.", "keywords": ["trim_messages", "max_tokens", "token_counter", "count_tokens_approximately", "strategy", "context window", "LangChain", "message history", "truncate", "StateGraph"]}