{"node_id": "a2b72b16", "title": "LLM tokens", "path": "streaming > Streaming > LLM tokens", "content": "Use the `messages` streaming mode to stream Large Language Model (LLM) outputs **token by token** from any part of your graph, including nodes, tools, subgraphs, or tasks.\n\nThe streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message_chunk, metadata)` where:\n\n* `message_chunk`: the token or message segment from the LLM.\n* `metadata`: a dictionary containing details about the graph node and LLM invocation.\n\n> If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.\n\n<Warning>\n  **Manual config required for async in Python \\< 3.11**\n  When using Python \\< 3.11 with async code, you must explicitly pass [`RunnableConfig`](https://reference.langchain.com/python/langchain_core/runnables/#langchain_core.runnables.RunnableConfig) to `ainvoke()` to enable proper streaming. See [Async with Python \\< 3.11](#async) for details or upgrade to Python 3.11+.\n</Warning>\n\n```python  theme={null}\nfrom dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nmodel = init_chat_model(model=\"gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    # Note that message events are emitted even when the LLM is run using .invoke rather than .stream\n    model_response = model.invoke(  # [!code highlight]\n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": model_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\n# The \"messages\" stream mode returns an iterator of tuples (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor message_chunk, metadata in graph.stream(\n    {\"topic\": \"ice cream\"},\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n```", "summary": "Stream LLM outputs token by token using 'messages' mode in LangGraph, with metadata for filtering by node or LLM invocation.", "keywords": ["messages", "stream_mode", "LLM tokens", "metadata", "StateGraph", "init_chat_model", "langgraph_node", "RunnableConfig", "ainvoke", "graph.stream"]}