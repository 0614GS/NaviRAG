{"node_id": "325a8665", "title": "Use semantic search", "path": "add-memory > Memory > Add long-term memory > Use semantic search", "content": "Enable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.\n\n```python  theme={null}\nfrom langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n```\n\n<Accordion title=\"Long-term memory with semantic search\">\n  ```python  theme={null}\n\n  from langchain.embeddings import init_embeddings\n  from langchain.chat_models import init_chat_model\n  from langgraph.store.base import BaseStore\n  from langgraph.store.memory import InMemoryStore\n  from langgraph.graph import START, MessagesState, StateGraph\n\n  model = init_chat_model(\"gpt-4o-mini\")\n\n  # Create store with semantic search enabled\n  embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n  store = InMemoryStore(\n      index={\n          \"embed\": embeddings,\n          \"dims\": 1536,\n      }\n  )\n\n  store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n  store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\n  def chat(state, *, store: BaseStore):\n      # Search based on user's last message\n      items = store.search(\n          (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n      )\n      memories = \"\\n\".join(item.value[\"text\"] for item in items)\n      memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n      response = model.invoke(\n          [\n              {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n              *state[\"messages\"],\n          ]\n      )\n      return {\"messages\": [response]}\n\n\n  builder = StateGraph(MessagesState)\n  builder.add_node(chat)\n  builder.add_edge(START, \"chat\")\n  graph = builder.compile(store=store)\n\n  for message, metadata in graph.stream(\n      input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n      stream_mode=\"messages\",\n  ):\n      print(message.content, end=\"\")\n  ```\n</Accordion>", "summary": "Enables semantic search in a graph's memory store using embeddings to retrieve items by similarity, with code examples for setup and integration in a chat application.", "keywords": ["semantic search", "InMemoryStore", "embeddings", "init_embeddings", "store.search", "BaseStore", "StateGraph", "text-embedding-3-small", "graph agents", "memory store"]}