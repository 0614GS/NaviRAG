{"node_id": "c6bf038f", "title": "Connect to your agent", "path": "ui > Agent Chat UI > Connect to your agent", "content": "Agent Chat UI can connect to both [local](/oss/python/langgraph/studio#setup-local-agent-server) and [deployed agents](/oss/python/langgraph/deploy).\n\nAfter starting Agent Chat UI, you'll need to configure it to connect to your agent:\n\n1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)\n2. **Deployment URL**: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)\n3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local Agent server)\n\nOnce configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.\n\n<Tip>\n  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).\n</Tip>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/ui.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Instructions for connecting Agent Chat UI to local or deployed agents, including required configuration fields and optional LangSmith API key setup.", "keywords": ["Agent Chat UI", "Graph ID", "Deployment URL", "LangSmith API key", "local agent", "deployed agents", "interrupted threads", "tool calls", "tool result messages", "langgraph.json"]}