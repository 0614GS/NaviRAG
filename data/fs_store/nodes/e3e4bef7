{"node_id": "e3e4bef7", "title": "Filter by node", "path": "streaming > Streaming > LLM tokens > Filter by node", "content": "To stream tokens only from specific nodes, use `stream_mode=\"messages\"` and filter the outputs by the `langgraph_node` field in the streamed metadata:\n\n```python  theme={null}\n# The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n# where message_chunk is the token streamed by the LLM and metadata is a dictionary\n# with information about the graph node where the LLM was called and other information\nfor msg, metadata in graph.stream(\n    inputs,\n    stream_mode=\"messages\",  # [!code highlight]\n):\n    # Filter the streamed tokens by the langgraph_node field in the metadata\n    # to only include the tokens from the specified node\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\":\n        ...\n```\n\n<Accordion title=\"Extended example: streaming LLM tokens from specific nodes\">\n  ```python  theme={null}\n  from typing import TypedDict\n  from langgraph.graph import START, StateGraph\n  from langchain_openai import ChatOpenAI\n\n  model = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\n  class State(TypedDict):\n        topic: str\n        joke: str\n        poem: str\n\n\n  def write_joke(state: State):\n        topic = state[\"topic\"]\n        joke_response = model.invoke(\n              [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n        )\n        return {\"joke\": joke_response.content}\n\n\n  def write_poem(state: State):\n        topic = state[\"topic\"]\n        poem_response = model.invoke(\n              [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n        )\n        return {\"poem\": poem_response.content}\n\n\n  graph = (\n        StateGraph(State)\n        .add_node(write_joke)\n        .add_node(write_poem)\n        # write both the joke and the poem concurrently\n        .add_edge(START, \"write_joke\")\n        .add_edge(START, \"write_poem\")\n        .compile()\n  )\n\n  # The \"messages\" stream mode returns a tuple of (message_chunk, metadata)\n  # where message_chunk is the token streamed by the LLM and metadata is a dictionary\n  # with information about the graph node where the LLM was called and other information\n  for msg, metadata in graph.stream(\n      {\"topic\": \"cats\"},\n      stream_mode=\"messages\",  # [!code highlight]\n  ):\n      # Filter the streamed tokens by the langgraph_node field in the metadata\n      # to only include the tokens from the write_poem node\n      if msg.content and metadata[\"langgraph_node\"] == \"write_poem\":\n          print(msg.content, end=\"|\", flush=True)\n  ```\n</Accordion>", "summary": "Explains how to filter streaming LLM tokens by specific graph nodes using `stream_mode='messages'` and the `langgraph_node` metadata field.", "keywords": ["stream_mode", "messages", "langgraph_node", "metadata", "graph.stream", "LLM tokens", "filter", "node", "StateGraph", "ChatOpenAI"]}