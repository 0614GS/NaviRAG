{"node_id": "838c65de", "title": "4. Define agent", "path": "quickstart > Quickstart > 4. Define agent", "content": "The agent is built using the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) function.\n\n    <Note>\n      In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.\n    </Note>\n\n    ```python  theme={null}\n    @entrypoint()\n    def agent(messages: list[BaseMessage]):\n        model_response = call_llm(messages).result()\n\n        while True:\n            if not model_response.tool_calls:\n                break\n\n            # Execute tools\n            tool_result_futures = [\n                call_tool(tool_call) for tool_call in model_response.tool_calls\n            ]\n            tool_results = [fut.result() for fut in tool_result_futures]\n            messages = add_messages(messages, [model_response, *tool_results])\n            model_response = call_llm(messages).result()\n\n        messages = add_messages(messages, model_response)\n        return messages\n\n    # Invoke\n    messages = [HumanMessage(content=\"Add 3 and 4.\")]\n    for chunk in agent.stream(messages, stream_mode=\"updates\"):\n        print(chunk)\n        print(\"\\n\")\n    ```\n\n    <Tip>\n      To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).\n    </Tip>\n\n    Congratulations! You've built your first agent using the LangGraph Functional API.\n\n    <Accordion title=\"Full code example\" icon=\"code\">\n      ```python  theme={null}\n      # Step 1: Define tools and model\n\n      from langchain.tools import tool\n      from langchain.chat_models import init_chat_model\n\n\n      model = init_chat_model(\n          \"claude-sonnet-4-5-20250929\",\n          temperature=0\n      )\n\n\n      # Define tools\n      @tool\n      def multiply(a: int, b: int) -> int:\n          \"\"\"Multiply `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a * b\n\n\n      @tool\n      def add(a: int, b: int) -> int:\n          \"\"\"Adds `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a + b\n\n\n      @tool\n      def divide(a: int, b: int) -> float:\n          \"\"\"Divide `a` and `b`.\n\n          Args:\n              a: First int\n              b: Second int\n          \"\"\"\n          return a / b\n\n\n      # Augment the LLM with tools\n      tools = [add, multiply, divide]\n      tools_by_name = {tool.name: tool for tool in tools}\n      model_with_tools = model.bind_tools(tools)\n\n      from langgraph.graph import add_messages\n      from langchain.messages import (\n          SystemMessage,\n          HumanMessage,\n          ToolCall,\n      )\n      from langchain_core.messages import BaseMessage\n      from langgraph.func import entrypoint, task\n\n\n      # Step 2: Define model node\n\n      @task\n      def call_llm(messages: list[BaseMessage]):\n          \"\"\"LLM decides whether to call a tool or not\"\"\"\n          return model_with_tools.invoke(\n              [\n                  SystemMessage(\n                      content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                  )\n              ]\n              + messages\n          )\n\n\n      # Step 3: Define tool node\n\n      @task\n      def call_tool(tool_call: ToolCall):\n          \"\"\"Performs the tool call\"\"\"\n          tool = tools_by_name[tool_call[\"name\"]]\n          return tool.invoke(tool_call)\n\n\n      # Step 4: Define agent\n\n      @entrypoint()\n      def agent(messages: list[BaseMessage]):\n          model_response = call_llm(messages).result()\n\n          while True:\n              if not model_response.tool_calls:\n                  break\n\n              # Execute tools\n              tool_result_futures = [\n                  call_tool(tool_call) for tool_call in model_response.tool_calls\n              ]\n              tool_results = [fut.result() for fut in tool_result_futures]\n              messages = add_messages(messages, [model_response, *tool_results])\n              model_response = call_llm(messages).result()\n\n          messages = add_messages(messages, model_response)\n          return messages\n\n      # Invoke\n      messages = [HumanMessage(content=\"Add 3 and 4.\")]\n      for chunk in agent.stream(messages, stream_mode=\"updates\"):\n          print(chunk)\n          print(\"\\n\")\n      ```\n    </Accordion>\n  </Tab>\n</Tabs>\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Defines an agent using LangGraph's Functional API with @entrypoint, demonstrating tool calling and control flow within a single function.", "keywords": ["@entrypoint", "Functional API", "agent", "tool_calls", "call_llm", "call_tool", "BaseMessage", "LangGraph", "stream", "add_messages"]}